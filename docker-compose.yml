# docker-compose.yml - LexAI with Host-Based Ollama
# Ollama runs on host (already installed), LexAI connects to it

services:
  # ============================================================================
  # LEXAI APPLICATION
  # ============================================================================
  lexai-app:
    build:
      context: .
      dockerfile: Dockerfile
      args:
        INSTALL_LLM: "false"
        BUILD_ENV: "production"

    container_name: lexai-app

    ports:
      - "8501:8501"

    environment:
      # Streamlit configuration
      STREAMLIT_SERVER_ADDRESS: "0.0.0.0"
      STREAMLIT_SERVER_PORT: "8501"
      STREAMLIT_SERVER_HEADLESS: "true"
      STREAMLIT_BROWSER_GATHER_USAGE_STATS: "false"
      STREAMLIT_SERVER_MAX_UPLOAD_SIZE: "100"

      # Application settings
      PYTHONPATH: "/app"
      APP_NAME: "LexAI"
      APP_VERSION: "2.0.0"
      LOG_LEVEL: "INFO"
      DEBUG_MODE: "false"

      # LLM Configuration - Using Host Ollama
      COMPLIANCE_ENGINE: "llm"
      USE_LLM_ANALYZER: "true"
      USE_LOCAL_ENGINE: "false"
      DATA_DIR: "/app/data"

      # Ollama connection - Uses host.docker.internal for cross-platform support
      # For Linux: Uses extra_hosts mapping below (host.docker.internal -> host-gateway)
      # For Mac/Windows Docker Desktop: host.docker.internal works natively
      OLLAMA_HOST: "http://192.168.1.24:11434"
      OLLAMA_MODEL: "llama3.2:3b"
      LLM_ANALYSIS_MODE: "hybrid"
      LLM_TEMPERATURE: "0.1"
      LLM_MAX_TOKENS: "2000"
      OLLAMA_TIMEOUT: "120"

      # Performance optimization
      OMP_NUM_THREADS: "4"
      PYTHONUNBUFFERED: "1"
      PYTHONDONTWRITEBYTECODE: "1"

      # Security
      SESSION_SECRET_KEY: "${SESSION_SECRET_KEY:-change-me-in-production}"
      SECURE_MODE: "true"

    volumes:
      - ./data:/app/data:ro
      - lexai-logs:/app/logs

    restart: unless-stopped

    networks:
      - lexai-net

    # Allow container to reach host (required for Linux)
    # On Mac/Windows Docker Desktop, host.docker.internal works without this
    extra_hosts:
      - "host.docker.internal:host-gateway"

    # Resource limits
    mem_limit: 2g
    mem_reservation: 1g
    cpus: 1.0

    healthcheck:
      test: ["CMD", "curl", "-f", "http://192.168.124:8501/_stcore/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

# ============================================================================
# NETWORKS AND VOLUMES
# ============================================================================
networks:
  lexai-net:
    driver: bridge

volumes:
  lexai-logs:
    driver: local

# ============================================================================
# USAGE NOTES
# ============================================================================
#
# PREREQUISITES:
# 1. Install Ollama on host: curl -fsSL https://ollama.com/install.sh | sh
# 2. Pull model: ollama pull llama3.2:3b
# 3. Ensure Ollama is running:
#    - Linux: systemctl start ollama
#    - Mac/Windows: Ollama app should be running
#
# VERIFY OLLAMA IS ACCESSIBLE:
#   curl http://localhost:11434/api/tags
#
# DEPLOY:
#   docker-compose up -d --build
#
# VERIFY FROM CONTAINER:
#   docker exec lexai-app curl http://host.docker.internal:11434/api/tags
#
# TROUBLESHOOTING:
# - If container can't reach Ollama on Linux, try:
#   OLLAMA_HOST: "http://172.17.0.1:11434"
# - On some Linux distros, you may need to configure Ollama to listen on all interfaces:
#   Edit /etc/systemd/system/ollama.service and add:
#   Environment="OLLAMA_HOST=0.0.0.0"
#   Then: systemctl daemon-reload && systemctl restart ollama