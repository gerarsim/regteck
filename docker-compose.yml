# docker-compose.yml - LexAI with Host-Based Ollama
# Ollama runs on host (already installed), LexAI connects to it

services:
  # ============================================================================
  # LEXAI APPLICATION
  # ============================================================================
  lexai-app:
    build:
      context: .
      dockerfile: Dockerfile
      args:
        INSTALL_LLM: "false"
        BUILD_ENV: "production"

    container_name: lexai-app

    ports:
      - "8501:8501"

    environment:
      # Streamlit configuration
      STREAMLIT_SERVER_ADDRESS: "0.0.0.0"
      STREAMLIT_SERVER_PORT: "8501"
      STREAMLIT_SERVER_HEADLESS: "true"
      STREAMLIT_BROWSER_GATHER_USAGE_STATS: "false"
      STREAMLIT_SERVER_MAX_UPLOAD_SIZE: "100"

      # Application settings
      PYTHONPATH: "/app"
      APP_NAME: "LexAI"
      APP_VERSION: "2.0.0"
      LOG_LEVEL: "INFO"
      DEBUG_MODE: "false"

      # ✅ LLM Configuration - Using Host Ollama
      COMPLIANCE_ENGINE: "llm"
      USE_LLM_ANALYZER: "true"
      USE_LOCAL_ENGINE: "false"
      DATA_DIR: "/app/data"

      # ✅ Ollama connection to host (Linux)
      OLLAMA_HOST: "http://172.17.0.1:11434"
      OLLAMA_MODEL: "llama3.2:3b"
      LLM_ANALYSIS_MODE: "hybrid"
      LLM_TEMPERATURE: "0.1"
      LLM_MAX_TOKENS: "2000"

      # Performance optimization
      OMP_NUM_THREADS: "4"
      PYTHONUNBUFFERED: "1"
      PYTHONDONTWRITEBYTECODE: "1"

      # Security
      SESSION_SECRET_KEY: "${SESSION_SECRET_KEY:-change-me-in-production}"
      SECURE_MODE: "true"

    volumes:
      - ./data:/app/data:ro
      - lexai-logs:/app/logs

    restart: unless-stopped

    networks:
      - lexai-net

    # ✅ Allow container to reach host (Linux)
    extra_hosts:
      - "host.docker.internal:host-gateway"

    # Resource limits
    mem_limit: 2g
    mem_reservation: 1g
    cpus: 1.0

    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8501/_stcore/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

# ============================================================================
# NETWORKS AND VOLUMES
# ============================================================================
networks:
  lexai-net:
    driver: bridge

volumes:
  lexai-logs:
    driver: local

# ============================================================================
# NOTES
# ============================================================================
# Ollama is running on the host machine at 127.0.0.1:11434
# The container accesses it via 172.17.0.1:11434 (default Docker bridge gateway)
#
# Before deploying:
# 1. Ensure Ollama is running: systemctl status ollama
# 2. Ensure model is downloaded: ollama list
# 3. Test API: curl http://localhost:11434/api/tags
#
# Deploy:
# docker-compose down && docker-compose up -d
#
# Verify:
# docker exec lexai-app curl http://172.17.0.1:11434/api/tags